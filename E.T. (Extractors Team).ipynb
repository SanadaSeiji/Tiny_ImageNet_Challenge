{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (100000, 64, 64, 3)\n",
      "100000 train samples\n",
      "10000 test samples\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:214: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:252: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "781/781 [==============================] - 482s 618ms/step - loss: 3.4007 - top_5_accuracy: 0.4967 - val_loss: 3.2653 - val_top_5_accuracy: 0.5296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:406: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 434us/step\n",
      "Test score: 3.265316176223755\n",
      "Test accuracy: 0.5296\n"
     ]
    }
   ],
   "source": [
    "############################load img############################################\n",
    "# load the val annotations file\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def get_annotations_map():\n",
    "    valAnnotationsPath = './tiny-imagenet-200/val/val_annotations.txt'\n",
    "    valAnnotationsFile = open(valAnnotationsPath, 'r')\n",
    "    valAnnotationsContents = valAnnotationsFile.read()\n",
    "    valAnnotations = {}\n",
    "\n",
    "    for line in valAnnotationsContents.splitlines():\n",
    "        pieces = line.strip().split()\n",
    "        valAnnotations[pieces[0]] = pieces[1]\n",
    "    \n",
    "    return valAnnotations\n",
    "\n",
    "\n",
    "#Sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_images(path,num_classes):\n",
    "    #Load images\n",
    "    \n",
    "    print('Loading ' + str(num_classes) + ' classes')\n",
    "\n",
    "    X_train0=np.zeros([num_classes*500,3,64,64],dtype='uint8')\n",
    "    X_train=np.zeros([num_classes*500,64,64,3],dtype='uint8')\n",
    "    y_train=np.zeros([num_classes*500], dtype='uint8')\n",
    "\n",
    "    trainPath=path+'/train'\n",
    "\n",
    "    print('loading training images...');\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    annotations={}\n",
    "    for sChild in os.listdir(trainPath):\n",
    "        sChildPath = os.path.join(os.path.join(trainPath,sChild),'images')\n",
    "        annotations[sChild]=j\n",
    "        for c in os.listdir(sChildPath):\n",
    "            X=np.array(Image.open(os.path.join(sChildPath,c)))\n",
    "            if len(np.shape(X))==2:\n",
    "                X_train0[i]=np.array([X,X,X])\n",
    "            else:\n",
    "                X_train0[i]=np.transpose(X,(2,0,1))\n",
    "            X_train[i]=np.transpose(X_train0[i],(1,2,0))\n",
    "            y_train[i]=j\n",
    "            i+=1\n",
    "        j+=1\n",
    "        if (j >= num_classes):\n",
    "            break\n",
    "\n",
    "    #print('finished loading training images')\n",
    "\n",
    "    val_annotations_map = get_annotations_map()\n",
    "\n",
    "    X_test0 = np.zeros([num_classes*50,3,64,64],dtype='uint8')\n",
    "    X_test = np.zeros([num_classes*50,64,64,3],dtype='uint8')\n",
    "    y_test = np.zeros([num_classes*50], dtype='uint8')\n",
    "\n",
    "\n",
    "    print('loading test images...')\n",
    "\n",
    "    i = 0\n",
    "    testPath=path+'/val/images'\n",
    "    for sChild in os.listdir(testPath):\n",
    "        if val_annotations_map[sChild] in annotations.keys():\n",
    "            sChildPath = os.path.join(testPath, sChild)\n",
    "            X=np.array(Image.open(sChildPath))\n",
    "            if len(np.shape(X))==2:\n",
    "                X_test0[i]=np.array([X,X,X])\n",
    "            else:\n",
    "                X_test0[i]=np.transpose(X,(2,0,1))\n",
    "            X_test[i]=np.transpose( X_test0[i],(1,2,0)) \n",
    "            y_test[i]=annotations[val_annotations_map[sChild]]\n",
    "            i+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "   # print('finished loading test images')+str(i)\n",
    "\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "############################load img###################################################\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"ResNet50 model for Keras.\n",
    "\n",
    "# Reference:\n",
    "\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "Adapted from code contributed by BigMoyan.\n",
    "\"\"\"\n",
    "\n",
    "############################load pretrained model: ResNet152####################################################\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Flatten, Activation, add\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "from keras.layers import merge\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(3000)\n",
    "\n",
    "class Scale(Layer):\n",
    "    '''Custom Layer for ResNet used for BatchNormalization.\n",
    "    \n",
    "    Learns a set of weights and biases used for scaling the input data.\n",
    "    the output consists simply in an element-wise multiplication of the input\n",
    "    and a sum of a set of constants:\n",
    "        out = in * gamma + beta,\n",
    "    where 'gamma' and 'beta' are the weights and biases larned.\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializers](../initializers.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializers](../initializers.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "    '''\n",
    "    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "        self.beta_init = initializers.get(beta_init)\n",
    "        self.gamma_init = initializers.get(gamma_init)\n",
    "        self.initial_weights = weights\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (int(input_shape[self.axis]),)\n",
    "\n",
    "        self.gamma = K.variable(self.gamma_init(shape), name='%s_gamma'%self.name)\n",
    "        self.beta = K.variable(self.beta_init(shape), name='%s_beta'%self.name)\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
    "        base_config = super(Scale, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def identity_blockX(input_tensor, filters, stage, block):\n",
    "    '''The identity_block is the block that has no conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters, (1, 1), name=conv_name_base + '2a', use_bias=False)(input_tensor)\n",
    "    #x = Dropout(0.2, name=conv_name_base + '2a_dropout')(x)\n",
    "   # x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    #x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (1, 1), name=conv_name_base + '2b', use_bias=False)(x)\n",
    "    #x = Dropout(0.2, name=conv_name_base + '2b_dropout')(x)\n",
    "    #x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    #x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Conv2D(16, (1, 1), name=conv_name_base + '2c', use_bias=False)(x) #???\n",
    "    #x = Dropout(0.2, name=conv_name_base + '2c_dropout')(x)\n",
    "    #x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    #x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "    \n",
    "\n",
    "    x = merge([x, input_tensor], mode='sum', name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x\n",
    "\n",
    "def identity_blockY(input_tensor, filters, stage, block):\n",
    "    '''The identity_block is the block that has no conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    \n",
    "    y = Conv2D(filters, (3, 3), name=conv_name_base + '3a', use_bias=False)(input_tensor)\n",
    "    #y = Dropout(0.2, name=conv_name_base + '3a_dropout')(y)\n",
    "    #y = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '3a')(y)\n",
    "    #y = Scale(axis=bn_axis, name=scale_name_base + '3a')(y)\n",
    "    y = Activation('relu', name=conv_name_base + '3a_relu')(y)\n",
    "    \n",
    "    y = ZeroPadding2D((3, 3), name=conv_name_base + '3b_zeropadding')(y)\n",
    "\n",
    "    y = Conv2D(filters, (3, 3), name=conv_name_base + '3c', use_bias=False)(y)\n",
    "    #y = Dropout(0.2, name=conv_name_base + '3c_dropout')(y)\n",
    "    #y = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '3c')(y)\n",
    "    #y = Scale(axis=bn_axis, name=scale_name_base + '3c')(y)\n",
    "    y = Activation('relu', name=conv_name_base + '3c_relu')(y)\n",
    "\n",
    "    y = Conv2D(16, (3, 3), name=conv_name_base + '3d', use_bias=False)(y) #???\n",
    "    #y = Dropout(0.2, name=conv_name_base + '3d_dropout')(y)\n",
    "   # y = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '3d')(y)\n",
    "    #y = Scale(axis=bn_axis, name=scale_name_base + '3d')(y)\n",
    "\n",
    "    x = merge([y, input_tensor], mode='sum', name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x\n",
    "\n",
    "def CrowdInceptron(include_top=False, weights=None,\n",
    "             input_tensor=None, input_shape=None,\n",
    "             pooling=None,\n",
    "             classes=20):\n",
    "    \"\"\"Instantiates the ResNet50 architecture.\n",
    "\n",
    "    \"\"\"\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global bn_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        bn_axis = 3\n",
    "        img_input = Input(shape=(64, 64, 3), name='data')\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "        img_input = Input(shape=(3, 64, 64), name='data')\n",
    "\n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
    "    x = Conv2D(16, (5, 5), strides=(1, 1), name='conv1', use_bias=False)(x)\n",
    "    #x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)\n",
    "    #x = Scale(axis=bn_axis, name='scale_conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(1, 1), name='pool1')(x)\n",
    "\n",
    "    #x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_blockX(x, 16, stage=2, block='b')\n",
    "    x = identity_blockY(x, 16, stage=2, block='c')\n",
    "\n",
    "    #x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_blockX(x, 16, stage=3, block='b')\n",
    "    x = identity_blockY(x, 16, stage=3, block='c')\n",
    "    \n",
    "\n",
    "    #x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_blockX(x, 16, stage=4, block='b')\n",
    "    x = identity_blockY(x, 16, stage=4, block='c')\n",
    "  \n",
    "    #x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_blockX(x, 16, stage=5, block='b')\n",
    "    x = identity_blockY(x, 16, stage=5, block='c')\n",
    "\n",
    "    x = MaxPooling2D((5, 5), name='max_pool')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu', name = 'fc00')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(200, activation='softmax', name = 'fc200')(x) # old class!!\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='resnet50')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                    WEIGHTS_PATH,\n",
    "                                    cache_subdir='models',\n",
    "                                    md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\n",
    "        else:\n",
    "            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    WEIGHTS_PATH_NO_TOP,\n",
    "                                    cache_subdir='models',\n",
    "                                    md5_hash='a268eb855778b3df3c7506639542a6af')\n",
    "        model.load_weights(weights_path)\n",
    "        if K.backend() == 'theano':\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "            if include_top:\n",
    "                maxpool = model.get_layer(name='avg_pool')\n",
    "                shape = maxpool.output_shape[1:]\n",
    "                dense = model.get_layer(name='fc1000')\n",
    "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
    "\n",
    "        if K.image_data_format() == 'channels_first' and K.backend() == 'tensorflow':\n",
    "            warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                          'are using the Theano '\n",
    "                          'image data format convention '\n",
    "                          '(`image_data_format=\"channels_first\"`). '\n",
    "                          'For best performance, set '\n",
    "                          '`image_data_format=\"channels_last\"` in '\n",
    "                          'your Keras config '\n",
    "                          'at ~/.keras/keras.json.')\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "#########################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    #Keras\n",
    "    import keras\n",
    "    from keras.datasets import mnist\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras.optimizers import RMSprop\n",
    "    #from keras.regularizers import WeightRegularizer, ActivityRegularizer \n",
    "    from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "    from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "    from keras.layers.normalization import BatchNormalization \n",
    "    from keras.utils import np_utils\n",
    "    from keras.optimizers import SGD, Adam\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    #from plotter import Plotter\n",
    "    # from keras.utils.visualize_util import plot\n",
    "    import h5py\n",
    "    from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "    from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "\n",
    "    #Params\n",
    "    num_classes = 200\n",
    "    batch_size = 128 # cannot be larger...\n",
    "    nb_epoch = 1\n",
    "\n",
    "    h5f = h5py.File('IT.10.ad-200.vggs2.result.h5','r')#!!!\n",
    "    X_train = h5f['xTrain'][:]\n",
    "    Y_train =  h5f['yTrain'][:]\n",
    "    X_test =  h5f['xTest'][:]\n",
    "    Y_test =  h5f['yTest'][:]\n",
    "    \n",
    "    h5f.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print X dimension\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "    \n",
    "    num_samples=len(X_train)\n",
    "\n",
    "     \n",
    "   \n",
    "    \n",
    "    print ('done')\n",
    "    ##################################\n",
    "    \n",
    "\n",
    "    #input_shape = (64, 64, 3)\n",
    "\n",
    "    model = CrowdInceptron()\n",
    "    #loading weight from previous training\n",
    "    model.load_weights('ep00.TI.ad-200.max.weights.best.hdf5')\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    #sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #rmsprop = RMSprop(lr=0.001)\n",
    "    #sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "    adam = Adam(lr = 1e-3 )\n",
    "    \n",
    "    #---------------------------\n",
    "    \n",
    "    #for top - 5 accuracy\n",
    "    def top_5_accuracy(y_true, y_pred):\n",
    "        return top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
    "    \n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[top_5_accuracy])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #data augm\n",
    "    datagen = ImageDataGenerator(\n",
    "                #zca_whitening = True,\n",
    "                rotation_range=1,\n",
    "                #width_shift_range=0.05,\n",
    "                #height_shift_range=0.05,\n",
    "                #shear_range=0.2,\n",
    "                #zoom_range=0.2,\n",
    "                #channel_shift_range=0.2,\n",
    "                #cval=0.4, very cumbersome\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                fill_mode='nearest'\n",
    "              )\n",
    "    datagen.fit(X_train)\n",
    "    \n",
    "    \n",
    "    #checkpoint\n",
    "    # checkpoint\n",
    "    filepath=\"ep00.TI.ad-200.max2.weights.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    \n",
    "    reduceLr = ReduceLROnPlateau(monitor='loss', patience=50, mode= 'auto', factor = 0.5)\n",
    "    callbacks_list = [checkpoint, reduceLr]\n",
    "    \n",
    "    #fit model\n",
    "    history = model.fit_generator(datagen.flow(X_train.astype('float32')/255, Y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                        epochs=nb_epoch, shuffle=True, callbacks=callbacks_list,\n",
    "                        validation_data=(X_test.astype('float32')/255, Y_test))\n",
    "\n",
    "\n",
    "    \n",
    "    #evaluate model\n",
    "    score = model.evaluate(X_test.astype('float32')/255, Y_test, verbose=1)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #----------------load history----------------------------------\n",
    "    h5f = h5py.File('IT.10.sgd1e-6.FU.reduce10.result.h5','r')\n",
    "    hist_acc0 = h5f['acc'][:]\n",
    "    hist_valAcc0 =  h5f['val_acc'][:]\n",
    "    hist_loss0 =  h5f['loss'][:]\n",
    "    hist_valLoss0 =  h5f['val_loss'][:]\n",
    "    h5f.close()\n",
    "    \n",
    "    \n",
    "    #--------------save history------------------------\n",
    "    \n",
    "    hist_acc = np.concatenate((hist_acc0, history.history['acc']), axis = 0)\n",
    "    hist_valAcc = np.concatenate( (hist_valAcc0, history.history['val_acc']) , axis = 0)\n",
    "    hist_loss = np.concatenate( ( hist_loss0, history.history['loss'])  , axis = 0)\n",
    "    hist_valLoss =  np.concatenate( (hist_valLoss0, history.history['val_loss']) , axis = 0)\n",
    "    \n",
    "   \n",
    "    hist_acc = history.history['acc']\n",
    "    hist_valAcc = history.history['val_acc']\n",
    "    hist_loss = history.history['loss']\n",
    "    hist_valLoss = history.history['val_loss']\n",
    "    \n",
    "    #create h5\n",
    "    h5f = h5py.File('IT.10.ad-200.max2.result.h5', 'w')\n",
    "    h5f.create_dataset('acc', data=hist_acc)\n",
    "    h5f.create_dataset('val_acc', data=hist_valAcc)\n",
    "    h5f.create_dataset('loss', data=hist_loss)\n",
    "    h5f.create_dataset('val_loss', data=hist_valLoss)\n",
    "   \n",
    "    \n",
    "    \n",
    "    h5f.close()\n",
    "    \n",
    "    \n",
    "    #-------------plot-------------------\n",
    "     #  \"Accuracy\"\n",
    "    plt.plot(hist_acc )\n",
    "    plt.plot(hist_valAcc)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # \"Loss\"\n",
    "    plt.plot(hist_loss)\n",
    "    plt.plot(hist_valLoss)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
