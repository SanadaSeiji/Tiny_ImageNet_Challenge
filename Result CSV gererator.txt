
############################load model E.T. (Sun and Moon)####################################################

# -*- coding: utf-8 -*-
import os
import cv2
import copy

from keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Flatten, Activation, add
from keras.optimizers import SGD
from keras.layers.normalization import BatchNormalization
from keras.models import Model
from keras import initializers
from keras.engine import Layer, InputSpec
from keras import backend as K
from keras.layers import merge
from keras.layers import Dropout

import sys
sys.setrecursionlimit(3000)

class Scale(Layer):
    '''Custom Layer for ResNet used for BatchNormalization.
    
    Learns a set of weights and biases used for scaling the input data.
    the output consists simply in an element-wise multiplication of the input
    and a sum of a set of constants:
        out = in * gamma + beta,
    where 'gamma' and 'beta' are the weights and biases larned.
    # Arguments
        axis: integer, axis along which to normalize in mode 0. For instance,
            if your input tensor has shape (samples, channels, rows, cols),
            set axis to 1 to normalize per feature map (channels axis).
        momentum: momentum in the computation of the
            exponential average of the mean and standard deviation
            of the data, for feature-wise normalization.
        weights: Initialization weights.
            List of 2 Numpy arrays, with shapes:
            `[(input_shape,), (input_shape,)]`
        beta_init: name of initialization function for shift parameter
            (see [initializers](../initializers.md)), or alternatively,
            Theano/TensorFlow function to use for weights initialization.
            This parameter is only relevant if you don't pass a `weights` argument.
        gamma_init: name of initialization function for scale parameter (see
            [initializers](../initializers.md)), or alternatively,
            Theano/TensorFlow function to use for weights initialization.
            This parameter is only relevant if you don't pass a `weights` argument.
    '''
    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):
        self.momentum = momentum
        self.axis = axis
        self.beta_init = initializers.get(beta_init)
        self.gamma_init = initializers.get(gamma_init)
        self.initial_weights = weights
        super(Scale, self).__init__(**kwargs)

    def build(self, input_shape):
        self.input_spec = [InputSpec(shape=input_shape)]
        shape = (int(input_shape[self.axis]),)

        self.gamma = K.variable(self.gamma_init(shape), name='%s_gamma'%self.name)
        self.beta = K.variable(self.beta_init(shape), name='%s_beta'%self.name)
        self.trainable_weights = [self.gamma, self.beta]

        if self.initial_weights is not None:
            self.set_weights(self.initial_weights)
            del self.initial_weights

    def call(self, x, mask=None):
        input_shape = self.input_spec[0].shape
        broadcast_shape = [1] * len(input_shape)
        broadcast_shape[self.axis] = input_shape[self.axis]

        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)
        return out

    def get_config(self):
        config = {"momentum": self.momentum, "axis": self.axis}
        base_config = super(Scale, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

def identity_blockX(input_tensor, filters, stage, block):

    eps = 1.1e-5
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'
    scale_name_base = 'scale' + str(stage) + block + '_branch'

    x = Conv2D(filters, (1, 1), name=conv_name_base + '2a', use_bias=False)(input_tensor)
    #x = Dropout(0.2, name=conv_name_base + '2a_dropout')(x)
    #x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)
    #x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)
    x = Activation('relu', name=conv_name_base + '2a_relu')(x)

    x = Conv2D(filters, (1, 1), name=conv_name_base + '2b', use_bias=False)(x)
    #x = Dropout(0.2, name=conv_name_base + '2b_dropout')(x)
    #x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)
    #x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)
    x = Activation('relu', name=conv_name_base + '2b_relu')(x)

    x = Conv2D(16, (1, 1), name=conv_name_base + '2c', use_bias=False)(x) #???
    #x = Dropout(0.2, name=conv_name_base + '2c_dropout')(x)
    #x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)
    #x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)
    

    x = merge([x, input_tensor], mode='sum', name='res' + str(stage) + block)
    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)
    return x

def identity_blockY(input_tensor, filters, stage, block):

    eps = 1.1e-5
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'
    scale_name_base = 'scale' + str(stage) + block + '_branch'

    
    y = Conv2D(filters, (3, 3), name=conv_name_base + '3a', use_bias=False)(input_tensor)
    #y = Dropout(0.2, name=conv_name_base + '3a_dropout')(y)
    #y = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '3a')(y)
    #y = Scale(axis=bn_axis, name=scale_name_base + '3a')(y)
    y = Activation('relu', name=conv_name_base + '3a_relu')(y)
    
    y = ZeroPadding2D((3, 3), name=conv_name_base + '3b_zeropadding')(y)

    y = Conv2D(filters, (3, 3), name=conv_name_base + '3c', use_bias=False)(y)
    #y = Dropout(0.2, name=conv_name_base + '3c_dropout')(y)
    #y = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '3c')(y)
    #y = Scale(axis=bn_axis, name=scale_name_base + '3c')(y)
    y = Activation('relu', name=conv_name_base + '3c_relu')(y)

    y = Conv2D(16, (3, 3), name=conv_name_base + '3d', use_bias=False)(y) #???
    #y = Dropout(0.2, name=conv_name_base + '3d_dropout')(y)
    #y = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '3d')(y)
    #y = Scale(axis=bn_axis, name=scale_name_base + '3d')(y)

    x = merge([y, input_tensor], mode='sum', name='res' + str(stage) + block)
    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)
    return x

def SM(include_top=False, weights=None,
             input_tensor=None, input_shape=None,
             pooling=None,
             classes=20):
   
    eps = 1.1e-5

    # Handle Dimension Ordering for different backends
    global bn_axis
    if K.image_dim_ordering() == 'tf':
        bn_axis = 3
        img_input = Input(shape=(64, 64, 3), name='data')
    else:
        bn_axis = 1
        img_input = Input(shape=(3, 64, 64), name='data')

    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)
    x = Conv2D(16, (5, 5), strides=(1, 1), name='conv1', use_bias=False)(x)
    #x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)
    #x = Scale(axis=bn_axis, name='scale_conv1')(x)
    x = Activation('relu', name='conv1_relu')(x)
    x = MaxPooling2D((3, 3), strides=(1, 1), name='pool1')(x)

    x = identity_blockX(x, 16, stage=2, block='b')
    x = identity_blockY(x, 16, stage=2, block='c')

    x = identity_blockX(x, 16, stage=3, block='b')
    x = identity_blockY(x, 16, stage=3, block='c')
    
    x = identity_blockX(x, 16, stage=4, block='b')
    x = identity_blockY(x, 16, stage=4, block='c')
  
    x = identity_blockX(x, 16, stage=5, block='b')
    x = identity_blockY(x, 16, stage=5, block='c')

    x = AveragePooling2D((5, 5), name='avg_pool')(x)
    x = Flatten()(x)
    x = Dense(1024, activation='relu', name = 'fc00')(x)
    x = Dropout(0.5)(x)
    x = Dense(200, activation='softmax', name = 'fc200')(x) # old class!!

    # Ensure that the model takes into account
    # any potential predecessors of `input_tensor`.
    if input_tensor is not None:
        inputs = get_source_inputs(input_tensor)
    else:
        inputs = img_input
    # Create model.
    model = Model(inputs, x, name='resnet50')

    # load weights
    if weights is not None:
        model.load_weights(weights)

    return model
#########################################################################################

if __name__ == "__main__":
    
    import matplotlib.pyplot as plt
    
    #Keras
    import keras
    from keras.datasets import mnist
    from keras.models import Sequential
    from keras.layers import Dense, Dropout
    from keras.optimizers import RMSprop
    #from keras.regularizers import WeightRegularizer, ActivityRegularizer 
    from keras.layers.core import Dense, Dropout, Activation, Flatten
    from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D
    from keras.layers.normalization import BatchNormalization 
    from keras.utils import np_utils
    from keras.optimizers import SGD, Adam
    from keras.preprocessing.image import ImageDataGenerator
    #from plotter import Plotter
    # from keras.utils.visualize_util import plot
    import h5py
    from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
    import pandas as pd

    
    #-------------------load test raw data----------------------------
    X_test0 = np.zeros([200 * 50, 3, 64, 64], dtype='uint8')
    X_test = np.zeros([200 * 50, 64, 64, 3], dtype='uint8')

    i = 0
    testPath='./test_data_raw'
    for sChild in os.listdir(testPath):
        sChildPath = os.path.join(testPath, sChild)
        X=np.array(Image.open(sChildPath))
        if len(np.shape(X))==2:
            X_test0[i]=np.array([X,X,X])
        else:
            X_test0[i]=np.transpose(X,(2,0,1))
        X_test[i]=np.transpose( X_test0[i],(1,2,0)) 
            
        i+=1
    
    #print X dimension
    
    print(X_test.shape[0], 'test samples')
   
    #normalization test data (to match training data)
    X_test =  X_test.astype('float32')
    X_test /= 255
     
     
    print ('done loading test img')
    
    ##################################
    

    #-----------loading model-----------------------------

    model = SM()
    #loading weight from previous training
    model.load_weights('ep00.TI.ad-200.9.weights.best.hdf5')
    # compile the model (should be done *after* setting layers to non-trainable)
    #sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)
    #rmsprop = RMSprop(lr=0.001)
    adam = Adam(lr = 1e-3 )
    
    #---------------------------
    
    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
    
    #------------------predict-----------------
    predictions = model.predict(X_test)
    
    print(predictions.shape)
    #print('First prediction:', predictions[0]) # should be num_class probabilities
    
    #save the predictions (in probabilities)
    predictions = pd.DataFrame(predictions).to_csv('pred1.csv')
    
    print ('done saving results')
    